<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://afletcher53.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://afletcher53.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-04T20:37:42+00:00</updated><id>https://afletcher53.github.io/feed.xml</id><title type="html">Deep Dive, A PhD in Fact Finding within the Medical Domain</title><subtitle>Separating Fact from Fiction in the World of Medical Research. </subtitle><entry><title type="html">Paper review: Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond</title><link href="https://afletcher53.github.io/blog/2024/Causal-Inferance-NLP/" rel="alternate" type="text/html" title="Paper review: Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"/><published>2024-04-03T00:00:00+00:00</published><updated>2024-04-03T00:00:00+00:00</updated><id>https://afletcher53.github.io/blog/2024/Causal-Inferance-NLP</id><content type="html" xml:base="https://afletcher53.github.io/blog/2024/Causal-Inferance-NLP/"><![CDATA[<p><strong>Introduction:</strong> As part of our NLP coursework, I read the paper <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00511/113490/Causal-Inference-in-Natural-Language-Processing">“Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond”</a>. This paper is highly relevant to my research interests in applying causal inference methods to text data. In this review, I will summarize the key ideas, discuss the strengths and weaknesses, and reflect on the implications for future work.</p> <p><strong>Key Points:</strong></p> <ul> <li>The paper aims to clarify the differences between causal inference and prediction tasks in NLP</li> <li>It introduces key concepts such as counterfactuals, average treatment effect (ATE), conditional average treatment effect (CATE), and assumptions like ignorability, positivity, and consistency</li> <li>The authors discuss the challenges of estimating causal effects with textual confounders and propose approaches like propensity score matching and data augmentation</li> </ul> <p><strong>Strengths:</strong></p> <ul> <li>Provides a clear introduction to causal inference concepts and their relevance to NLP</li> <li>Offers a balanced discussion of the shortcomings of purely predictive models and the potential benefits of causal approaches</li> <li>Includes helpful mathematical formulations and illustrative examples throughout</li> <li>The section on data augmentation with counterfactuals was especially insightful and applicable to my work</li> </ul> <p><strong>Weaknesses:</strong></p> <ul> <li>The introduction could better clarify the distinction between causal inference and prediction upfront</li> <li>Some examples, such as the gendered avatar experiment, felt outdated and insensitive</li> <li>The discussion of the training-test distributional shift was unclear and unsupported; this is a known issue in the field</li> <li>Overuses the “black box” characterization of deep learning models without nuance</li> </ul> <p><strong>Suggestions for Improvement:</strong></p> <ul> <li>Provide a crisper definition of causal inference and prediction in the introduction and use more current examples</li> <li>Avoid broad generalizations about deep learning models and acknowledge ongoing work on interpretability</li> <li>Expand the discussion of potential applications and provide more concrete guidance for practitioners</li> </ul> <p><strong>Conclusion:</strong> Overall, this paper provides a valuable overview of causal inference methods for NLP and highlights essential challenges and future directions. Despite some areas for improvement in framing and examples, the authors make a compelling case for the benefits of causal approaches over purely predictive ones. The mathematical formulations and methodological suggestions will undoubtedly be helpful references for researchers working on related problems.</p> <p><strong>Reflection:</strong> Reading this paper has made me think more deeply about the limitations of the predictive models I currently use in my research and the potential for causal inference techniques to address issues of robustness, fairness, and interpretability. I am particularly eager to explore the idea of counterfactual data augmentation to improve model performance and mitigate spurious correlations.</p>]]></content><author><name>Aaron Fletcher</name></author><category term="Paper Review"/><category term="Journal Club"/><category term="Journal"/><category term="Paper"/><category term="Causality"/><category term="NLP"/><category term="LLMs"/><summary type="html"><![CDATA[A paper on how causality not correlation should be used to advance NLP]]></summary></entry><entry><title type="html">Pragmatics &amp;amp; NLI within LLMs</title><link href="https://afletcher53.github.io/blog/2024/Pragmatics-NLI-LLMs/" rel="alternate" type="text/html" title="Pragmatics &amp;amp; NLI within LLMs"/><published>2024-04-03T00:00:00+00:00</published><updated>2024-04-03T00:00:00+00:00</updated><id>https://afletcher53.github.io/blog/2024/Pragmatics-NLI-LLMs</id><content type="html" xml:base="https://afletcher53.github.io/blog/2024/Pragmatics-NLI-LLMs/"><![CDATA[<p>Today I wanted to share some thoughts on this paper, which I am reading as part of a small review on pragmatics within LLMs for a project that I am working on for over the next few months.</p> <p>The paper - When Truth Matters - Addressing Pragmatic Categories in Natural Language Inference (NLI) by large language models (LLMs) is a short article that attempts to do 4 things - Firstly detailing the notion of inference, secondly assess the extent of the phenomenon of non-assertive premises on a dataset, thirdly show how to ‘acquaint’ LLMs with these pragmatic categories and finally create a dataset for future researchers.</p> <p>I want to start out with that I am <strong><em>not</em></strong> a pragmatist by trade, and often a lot of these concepts are new to me (outside of knowing speech acts)! I do find the text surrounding pragmatics to be rather terminological dense, and that forms why I chose this paper to read - introduce me to some of the terminology used and also look at how people are applying these to large langauge models.</p> <p><strong>Introduction</strong></p> <p>The introduction section serves it purpose well, delivering key definitions that are used throughout the paper: Entailment - where comitting to a truth of one statement commits you to the truth of others</p> <p>Inference, which can be divided into two forms, deductively valid references (where it is not logically possible that a premise is true while the conclusion is false), or inductively valid inferences, where is possible that the premise is true while the conclusion is false, but where the truth of the premise in general is a good reason for the truth of the conclusion. They further develop this by stating that for two utterences to entail (or contradict) each other they need to be of the correct pragmatic category. The authors don’t make clear what a pragmatic category is and I cant find much with my customary google search.</p> <p>They go on to state how questions and commands (unlike assertions or claims) do not abide by these rules as they do not involve making a claim that could be true or false, so they cannot contradict or entail each other. For example “Did Loral Harm National security” doesn’t entail the speaker that national security is harmed. They use another example “Happy Hanukkah, everybody” which they claim does not bind the speaker to the state of affairs, however I am not quite convinced by this, as surely by saying this you are commiting yourself to the fact that it is Hanukkah (But again bear in mind I do not know enough about pragmatics to verify this).</p> <p>I particularly enjoyed the justification given about why distinguishing between the types of pragmatic kinds of utterence is important, and it is something that will hopefully help me with my overall PhD topic: Fact checking needs to be able to distinguish between claims and statements vs questions as one does not commit the speaker to entailment. From my mind, this is potentially applicable in identifying when someone tries to state a fact, verses when they are searching for information. Additionally the justification of LLMs needing reasoning capabilities to distinguish between fact and fiction to prevent misinformation propegation was an important one.</p> <p>The authors go on to frame the way in which LLMs are taught in NLI, using two sentences one is the premise (P), the other is the hypothesis (H) and the LLM then predicts contradiction of these two sentences (contradiction, P and H cannot both be true, entailment: P being correct means H is correct, or neither). They go on to outline the MLNI dataset:</p> <p>(P) yes now you know if if everybody like in August when everybody’s on vacation or something we can dress a little more casual or (H) August is a black out month for vacations in the company. = Contradiction</p> <p>(P) At the other end of Pennsylvania Avenue, people began to line up for a White House tour. (H) People formed a line at the end of Pennsylvania Avenue. = entailment</p> <p>They point out that two large datasets dominate the field MNLI, and also SNLI (an image pair caption dataset) and their respective bias. The MNLI dataset, for example, when having a negation in the hypothesis is more likely to be a contradiction, however they do not state the magnitude of this bias (while the referencing paper does, I would have liked this spelt out!). They outline how they adapt work by Williamson.</p> <p>We finally get to the “meat” of the paper (at page 6!) where they outline their main experiment. They hypothesis that :</p> <ul> <li>models fine tuned on MNLI are insensitive to non assertive premises cannot entail or be contradicted by other premise</li> <li>This “deficit” can be amended using properly composed fine-tuning dataset</li> <li>This does not significantly harm performance on the original MNLI dataset.</li> </ul> <p>3 transformer based models were used that had been fine tuned on MNLI (DeBERTa-base, XLNET-base, and RoBERTa-Large). For me, very little justification was given as to why these three models were chosen besides “they have different architectures”. Which, yes, they do, but I would have liked some discussion as to why the author’s felt the difference might have resulted in differences in the output.</p> <p>Its getting a little long for me, so I will continue in a second part!</p> <p>To be continued…</p>]]></content><author><name>Aaron Fletcher</name></author><category term="Paper Review"/><category term="Journal"/><category term="Paper"/><category term="Pragmatics"/><category term="LLMs"/><category term="NLI"/><summary type="html"><![CDATA[A paper on how LLMs handle different pragmatic sentence types]]></summary></entry></feed>